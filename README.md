# RIS Probe-Based Control with Limited Probing

A machine learning framework for optimizing Reconfigurable Intelligent Surface (RIS) phase configurations using limited probe measurements.

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.12+-red.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

## ðŸŽ¯ Problem Overview

In RIS-assisted wireless communications:
- **N** RIS elements (reflective surface) - default: 32
- **K** pre-defined probe configurations (phase patterns) - default: 64
- **M** sensing budget (probes we can actually measure) - default: 8

**Challenge**: Predict the best probe among all K options while only observing M << K measurements.

**Our Solution**: Train a neural network to learn patterns from limited observations and predict the optimal probe configuration.

## ðŸ—ï¸ Architecture

```
Input: [masked_powers, binary_mask] âˆˆ â„^{2K}
      â†“
   MLP (512 â†’ 256 â†’ 128)
      â†“
Output: logits âˆˆ â„^K (probability over all probes)
```

The model uses a **Masked Vector Approach**:
- `masked_powers`: K-dimensional vector with observed powers at their indices, zeros elsewhere
- `binary_mask`: K-dimensional vector with 1s at observed indices, 0s elsewhere

## ðŸ“¦ Installation

```bash
# Clone the repository
git clone https://github.com/mohbagher/Probe_Based_ML_Session5.git
cd Probe_Based_ML_Session5

# Install dependencies
pip install -r requirements.txt
```

### Dependencies
- Python â‰¥ 3.8
- PyTorch â‰¥ 1.12.0
- NumPy â‰¥ 1.21.0
- Matplotlib â‰¥ 3.5.0
- Seaborn â‰¥ 0.11.0
- SciPy â‰¥ 1.7.0
- tqdm â‰¥ 4.62.0
- pandas â‰¥ 1.3.0

## ðŸš€ Quick Start

### Option 1: Interactive Menu (Recommended)

```bash
python experiment_runner.py
```

This opens an interactive menu where you can:
- Select individual tasks (1-14)
- Run all tasks (0)
- Change parameters (S)
- Custom task selection (99)

When running multiple tasks in sequence, the runner waits 5 seconds between tasks (press Enter to skip the wait).
Training tasks use standardized default data sizes and epochs so comparisons are fair across tasks.

### Option 2: Command Line

```bash
# Run specific tasks
python experiment_runner.py --task 1,3,6 --N 32 --K 64 --M 8

# Run all tasks
python experiment_runner.py --task 0

# Custom parameters
python experiment_runner.py --task 6 --N 64 --K 128 --M 16 --seed 42
```

### Option 3: Python API

```python
from experiments.tasks.task_a1_binary import run_task_a1
from experiments.tasks.task_b1_m_variation import run_task_b1

# Run binary probe analysis
result = run_task_a1(N=32, K=64, M=8, seed=42)

# Run M variation study
result = run_task_b1(N=32, K=64, M=8, seed=42)
```

### Option 4: Original Training Script

```bash
python main.py --N 32 --K 64 --M 8 --epochs 100
```

## ðŸ“‹ Experiment Tasks

The framework includes 14 organized research tasks:

### Phase A: Probe Design
| Task | Command | Description |
|------|---------|-------------|
| A1 | `--task 1` | Binary (1-bit) probe generation and analysis |
| A2 | `--task 2` | Hadamard structured probe patterns |
| A3 | `--task 3` | Probe diversity comparison across all types |
| A4 | `--task 4` | Sobol low-discrepancy probe analysis |
| A5 | `--task 5` | Halton low-discrepancy probe analysis |

### Phase B: Limited Probing Analysis
| Task | Command | Description |
|------|---------|-------------|
| B1 | `--task 6` | M variation study (M âˆˆ {2,4,8,16,32}) |
| B2 | `--task 7` | Top-m selection performance |
| B3 | `--task 8` | ML vs baseline comparison |

### Phase C: Scaling Study
| Task | Command | Description |
|------|---------|-------------|
| C1 | `--task 9` | Scale K (number of probes) |
| C2 | `--task 10` | Phase resolution comparison |

### Phase D: Quality Control
| Task | Command | Description |
|------|---------|-------------|
| D1 | `--task 11` | Seed variation (reproducibility) |
| D2 | `--task 12` | Training sanity checks |

### Phase E: Documentation
| Task | Command | Description |
|------|---------|-------------|
| E1 | `--task 13` | Generate one-page summary |
| E2 | `--task 14` | Generate comparison plots |

## ðŸ“Š Key Metrics

- **Î· (eta)**: Power ratio = P_selected / P_best_probe
  - Î· = 1.0 â†’ Perfect (found the best probe)
  - Î· = 0.5 â†’ Selected probe gives half the optimal power

- **Top-m Accuracy**: Fraction where oracle best probe is in top-m predictions

- **Baselines**:
  - Random 1/K: Pick 1 probe randomly
  - Random M/K: Best of random M probes
  - Best Observed: Best among actually measured M probes
  - Oracle: Theoretical best (Î· = 1.0)

## ðŸ“ Project Structure

```
Probe_Based_ML_Session5/
â”œâ”€â”€ experiment_runner.py      # ðŸŽ¯ Main interactive experiment runner
â”œâ”€â”€ main.py                   # Original training script
â”œâ”€â”€ run.py                    # Quick experiment scripts
â”‚
â”œâ”€â”€ config.py                 # Configuration dataclasses
â”œâ”€â”€ model.py                  # MLP neural network architecture
â”œâ”€â”€ training.py               # Training loop with early stopping
â”œâ”€â”€ evaluation.py             # Metrics computation & baselines
â”œâ”€â”€ data_generation.py        # Channel simulation & datasets
â”œâ”€â”€ utils.py                  # Visualization & utilities
â”‚
â”œâ”€â”€ experiments/              # ðŸ§ª Experiment framework
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ probe_generators.py   # All probe types (continuous, binary, Hadamard)
â”‚   â”œâ”€â”€ diversity_analysis.py # Diversity metrics
â”‚   â””â”€â”€ tasks/                # Individual task implementations
â”‚       â”œâ”€â”€ task_a1_binary.py
â”‚       â”œâ”€â”€ task_a2_hadamard.py
â”‚       â”œâ”€â”€ task_a3_diversity.py
â”‚       â”œâ”€â”€ task_a4_sobol.py
â”‚       â”œâ”€â”€ task_a5_halton.py
â”‚       â”œâ”€â”€ task_b1_m_variation.py
â”‚       â”œâ”€â”€ task_b2_top_m.py
â”‚       â”œâ”€â”€ task_b3_baselines.py
â”‚       â””â”€â”€ ... (more tasks)
â”‚
â”œâ”€â”€ results/                  # ðŸ“ˆ Output directory (auto-created)
â”‚   â”œâ”€â”€ A1_binary_probes/
â”‚   â”œâ”€â”€ A2_hadamard_probes/
â”‚   â”œâ”€â”€ B1_M_variation/
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ EXPERIMENT_RUNNER.md      # Detailed experiment documentation
â”œâ”€â”€ IMPLEMENTATION_SUMMARY.md # Technical implementation details
â”œâ”€â”€ requirements.txt          # Dependencies
â”œâ”€â”€ LICENSE                   # MIT License
â””â”€â”€ README.md                 # This file
```

## ðŸ“ˆ Results Structure

Each task saves results to its own folder:

```
results/
â”œâ”€â”€ A1_binary_probes/
â”‚   â”œâ”€â”€ plots/
â”‚   â”‚   â”œâ”€â”€ phase_heatmap.png
â”‚   â”‚   â””â”€â”€ phase_histogram.png
â”‚   â””â”€â”€ metrics.txt
â”œâ”€â”€ B1_M_variation/
â”‚   â”œâ”€â”€ plots/
â”‚   â”‚   â””â”€â”€ eta_vs_M.png
â”‚   â””â”€â”€ metrics.txt
â””â”€â”€ comparison_all_models/
    â”œâ”€â”€ master_comparison.png
    â””â”€â”€ summary.txt
```

## ðŸ”¬ Probe Types

The framework supports 4 probe types:

| Type | Phases | Description | Use Case |
|------|--------|-------------|----------|
| Continuous | [0, 2Ï€) | Random phases | Baseline, theoretical limit |
| Binary (1-bit) | {0, Ï€} | Two-level quantization | Simple hardware |
| 2-bit | {0, Ï€/2, Ï€, 3Ï€/2} | Four-level quantization | Balanced complexity |
| Hadamard | {0, Ï€} structured | Orthogonal patterns | Maximum diversity |
| Sobol | [0, 2Ï€) low-discrepancy | Quasi-random coverage | Uniform space-filling |
| Halton | [0, 2Ï€) low-discrepancy | Quasi-random coverage | Uniform space-filling |

## ðŸ’» Command Line Arguments

| Argument | Default | Description |
|----------|---------|-------------|
| `--task` | None | Task(s) to run (1-14, 0=all, or comma-separated) |
| `--N` | 32 | Number of RIS elements |
| `--K` | 64 | Total probes in bank |
| `--M` | 8 | Sensing budget |
| `--seed` | 42 | Random seed |
| `--results-dir` | results | Base directory for outputs |

## ðŸ“š Documentation

- **[EXPERIMENT_RUNNER.md](EXPERIMENT_RUNNER.md)**: Complete guide to the experiment framework
- **[IMPLEMENTATION_SUMMARY.md](IMPLEMENTATION_SUMMARY.md)**: Technical implementation details

## ðŸŽ“ For PhD Research

### Recommended Experiment Sequence

1. **Quick validation**: `python experiment_runner.py --task 1`
2. **Probe design analysis**: `python experiment_runner.py --task 1,2,3,4,5`
3. **Core ML experiments**: `python experiment_runner.py --task 6,7,8`
4. **Full study**: `python experiment_runner.py --task 0`

### Key Results for Publications

- **Î· vs M curve**: Shows performance vs measurement overhead
- **ML vs Baselines**: Demonstrates ML advantage
- **Probe diversity**: Justifies structured probe design

## ðŸ“„ Citation

If you use this code in your research, please cite:

```bibtex
@software{ris_probe_ml,
  author = {Bagher, Mohammad},
  title = {RIS Probe-Based Control with Limited Probing},
  year = {2024},
  url = {https://github.com/mohbagher/Probe_Based_ML_Session5}
}
```

## ðŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ðŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ðŸ“§ Contact

For questions or collaboration, please open an issue on GitHub.
