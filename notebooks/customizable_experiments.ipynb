{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visual Experiment IDE (Fully Customizable)\n",
        "\n",
        "Use the UI below to configure **all parameters** in the project, choose probe-bank methods (random / Hadamard / Sobol / Halton), select models, and render plots.\n",
        "\n",
        "**Tip:** If widgets are missing, install them with `pip install ipywidgets` and restart the kernel.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "from config import get_config\n",
        "from data_generation import generate_probe_bank, create_dataloaders\n",
        "from training import train\n",
        "from evaluation import evaluate_model\n",
        "from utils import (\n",
        "    plot_training_history,\n",
        "    plot_eta_distribution,\n",
        "    plot_top_m_comparison,\n",
        "    plot_baseline_comparison,\n",
        ")\n",
        "from model import LimitedProbingMLP\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ---- DEFAULT CONFIG (all parameters exposed) ----\n",
        "DEFAULT_CONFIG = {\n",
        "    'system': {\n",
        "        'N': 32,\n",
        "        'K': 64,\n",
        "        'M': 8,\n",
        "        'P_tx': 1.0,\n",
        "        'sigma_h_sq': 1.0,\n",
        "        'sigma_g_sq': 1.0,\n",
        "        'phase_mode': 'continuous',  # continuous / discrete\n",
        "        'phase_bits': 3,\n",
        "        'probe_bank_method': 'random',  # random / hadamard / sobol / halton\n",
        "    },\n",
        "    'data': {\n",
        "        'n_train': 3000,\n",
        "        'n_val': 600,\n",
        "        'n_test': 600,\n",
        "        'seed': 42,\n",
        "        'normalize_input': True,\n",
        "        'normalization_type': 'mean',\n",
        "    },\n",
        "    'model': {\n",
        "        'hidden_sizes': [256, 128],\n",
        "        'dropout_prob': 0.1,\n",
        "        'use_batch_norm': True,\n",
        "    },\n",
        "    'training': {\n",
        "        'batch_size': 128,\n",
        "        'learning_rate': 1e-3,\n",
        "        'weight_decay': 1e-5,\n",
        "        'num_epochs': 10,\n",
        "        'early_stopping_patience': 15,\n",
        "        'eval_interval': 1,\n",
        "        'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    },\n",
        "    'eval': {\n",
        "        'top_m_values': [1, 2, 4, 8],\n",
        "    },\n",
        "    'probe_bank_methods': ['random'],\n",
        "    'models_to_run': ['mlp_default', 'mlp_shallow', 'linear'],\n",
        "    'plots': [\n",
        "        'training_history',\n",
        "        'eta_distribution',\n",
        "        'top_m_comparison',\n",
        "        'baseline_comparison',\n",
        "        'model_metric_bars',\n",
        "        'eta_boxplot',\n",
        "    ],\n",
        "    'compare_metrics': ['accuracy_top1', 'eta_top1', 'eta_top2'],\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ---- MODEL REGISTRY ----\n",
        "def build_mlp_default(config):\n",
        "    return LimitedProbingMLP(\n",
        "        K=config.system.K,\n",
        "        hidden_sizes=config.model.hidden_sizes,\n",
        "        dropout_prob=config.model.dropout_prob,\n",
        "        use_batch_norm=config.model.use_batch_norm,\n",
        "    )\n",
        "\n",
        "def build_mlp_shallow(config):\n",
        "    return LimitedProbingMLP(\n",
        "        K=config.system.K,\n",
        "        hidden_sizes=[128],\n",
        "        dropout_prob=0.0,\n",
        "        use_batch_norm=False,\n",
        "    )\n",
        "\n",
        "def build_linear(config):\n",
        "    input_size = 2 * config.system.K\n",
        "    output_size = config.system.K\n",
        "    return torch.nn.Sequential(torch.nn.Linear(input_size, output_size))\n",
        "\n",
        "MODEL_REGISTRY = {\n",
        "    'mlp_default': build_mlp_default,\n",
        "    'mlp_shallow': build_mlp_shallow,\n",
        "    'linear': build_linear,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ---- UI BUILDERS ----\n",
        "def _list_to_str(values):\n",
        "    return ','.join(str(v) for v in values)\n",
        "\n",
        "def _str_to_int_list(text):\n",
        "    return [int(v.strip()) for v in text.split(',') if v.strip()]\n",
        "\n",
        "system_widgets = {\n",
        "    'N': widgets.IntText(value=DEFAULT_CONFIG['system']['N'], description='N'),\n",
        "    'K': widgets.IntText(value=DEFAULT_CONFIG['system']['K'], description='K'),\n",
        "    'M': widgets.IntText(value=DEFAULT_CONFIG['system']['M'], description='M'),\n",
        "    'P_tx': widgets.FloatText(value=DEFAULT_CONFIG['system']['P_tx'], description='P_tx'),\n",
        "    'sigma_h_sq': widgets.FloatText(value=DEFAULT_CONFIG['system']['sigma_h_sq'], description='sigma_h_sq'),\n",
        "    'sigma_g_sq': widgets.FloatText(value=DEFAULT_CONFIG['system']['sigma_g_sq'], description='sigma_g_sq'),\n",
        "    'phase_mode': widgets.Dropdown(options=['continuous', 'discrete'], value=DEFAULT_CONFIG['system']['phase_mode'], description='phase_mode'),\n",
        "    'phase_bits': widgets.IntText(value=DEFAULT_CONFIG['system']['phase_bits'], description='phase_bits'),\n",
        "    'probe_bank_method': widgets.Dropdown(options=['random', 'hadamard', 'sobol', 'halton'], value=DEFAULT_CONFIG['system']['probe_bank_method'], description='bank_method'),\n",
        "}\n",
        "\n",
        "data_widgets = {\n",
        "    'n_train': widgets.IntText(value=DEFAULT_CONFIG['data']['n_train'], description='n_train'),\n",
        "    'n_val': widgets.IntText(value=DEFAULT_CONFIG['data']['n_val'], description='n_val'),\n",
        "    'n_test': widgets.IntText(value=DEFAULT_CONFIG['data']['n_test'], description='n_test'),\n",
        "    'seed': widgets.IntText(value=DEFAULT_CONFIG['data']['seed'], description='seed'),\n",
        "    'normalize_input': widgets.Checkbox(value=DEFAULT_CONFIG['data']['normalize_input'], description='normalize_input'),\n",
        "    'normalization_type': widgets.Dropdown(options=['mean', 'std', 'log'], value=DEFAULT_CONFIG['data']['normalization_type'], description='normalization_type'),\n",
        "}\n",
        "\n",
        "model_widgets = {\n",
        "    'hidden_sizes': widgets.Text(value=_list_to_str(DEFAULT_CONFIG['model']['hidden_sizes']), description='hidden_sizes'),\n",
        "    'dropout_prob': widgets.FloatText(value=DEFAULT_CONFIG['model']['dropout_prob'], description='dropout_prob'),\n",
        "    'use_batch_norm': widgets.Checkbox(value=DEFAULT_CONFIG['model']['use_batch_norm'], description='use_batch_norm'),\n",
        "}\n",
        "\n",
        "training_widgets = {\n",
        "    'batch_size': widgets.IntText(value=DEFAULT_CONFIG['training']['batch_size'], description='batch_size'),\n",
        "    'learning_rate': widgets.FloatText(value=DEFAULT_CONFIG['training']['learning_rate'], description='learning_rate'),\n",
        "    'weight_decay': widgets.FloatText(value=DEFAULT_CONFIG['training']['weight_decay'], description='weight_decay'),\n",
        "    'num_epochs': widgets.IntText(value=DEFAULT_CONFIG['training']['num_epochs'], description='num_epochs'),\n",
        "    'early_stopping_patience': widgets.IntText(value=DEFAULT_CONFIG['training']['early_stopping_patience'], description='early_stopping_patience'),\n",
        "    'eval_interval': widgets.IntText(value=DEFAULT_CONFIG['training']['eval_interval'], description='eval_interval'),\n",
        "    'device': widgets.Dropdown(options=['cpu', 'cuda'], value=DEFAULT_CONFIG['training']['device'], description='device'),\n",
        "}\n",
        "\n",
        "eval_widgets = {\n",
        "    'top_m_values': widgets.Text(value=_list_to_str(DEFAULT_CONFIG['eval']['top_m_values']), description='top_m_values'),\n",
        "}\n",
        "\n",
        "comparison_widgets = {\n",
        "    'probe_bank_methods': widgets.SelectMultiple(\n",
        "        options=['random', 'hadamard', 'sobol', 'halton'],\n",
        "        value=tuple(DEFAULT_CONFIG['probe_bank_methods']),\n",
        "        description='probe_methods'\n",
        "    ),\n",
        "    'models_to_run': widgets.SelectMultiple(\n",
        "        options=sorted(MODEL_REGISTRY.keys()),\n",
        "        value=tuple(DEFAULT_CONFIG['models_to_run']),\n",
        "        description='models'\n",
        "    ),\n",
        "    'plots': widgets.SelectMultiple(\n",
        "        options=[\n",
        "            'training_history', 'eta_distribution', 'top_m_comparison',\n",
        "            'baseline_comparison', 'model_metric_bars', 'eta_boxplot',\n",
        "        ],\n",
        "        value=tuple(DEFAULT_CONFIG['plots']),\n",
        "        description='plots'\n",
        "    ),\n",
        "    'compare_metrics': widgets.Text(value=_list_to_str(DEFAULT_CONFIG['compare_metrics']), description='compare_metrics'),\n",
        "}\n",
        "\n",
        "def _as_box(widget_dict):\n",
        "    return widgets.VBox(list(widget_dict.values()))\n",
        "\n",
        "tabs = widgets.Tab()\n",
        "tab_children = [\n",
        "    _as_box(system_widgets),\n",
        "    _as_box(data_widgets),\n",
        "    _as_box(model_widgets),\n",
        "    _as_box(training_widgets),\n",
        "    _as_box(eval_widgets),\n",
        "    _as_box(comparison_widgets),\n",
        "]\n",
        "tabs.children = tab_children\n",
        "tab_titles = ['System', 'Data', 'Model', 'Training', 'Eval', 'Compare/Plots']\n",
        "for i, title in enumerate(tab_titles):\n",
        "    tabs.set_title(i, title)\n",
        "\n",
        "display(tabs)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ---- CONFIG BUILDER ----\n",
        "def build_config_from_widgets():\n",
        "    return {\n",
        "        'system': {\n",
        "            'N': system_widgets['N'].value,\n",
        "            'K': system_widgets['K'].value,\n",
        "            'M': system_widgets['M'].value,\n",
        "            'P_tx': system_widgets['P_tx'].value,\n",
        "            'sigma_h_sq': system_widgets['sigma_h_sq'].value,\n",
        "            'sigma_g_sq': system_widgets['sigma_g_sq'].value,\n",
        "            'phase_mode': system_widgets['phase_mode'].value,\n",
        "            'phase_bits': system_widgets['phase_bits'].value,\n",
        "            'probe_bank_method': system_widgets['probe_bank_method'].value,\n",
        "        },\n",
        "        'data': {\n",
        "            'n_train': data_widgets['n_train'].value,\n",
        "            'n_val': data_widgets['n_val'].value,\n",
        "            'n_test': data_widgets['n_test'].value,\n",
        "            'seed': data_widgets['seed'].value,\n",
        "            'normalize_input': data_widgets['normalize_input'].value,\n",
        "            'normalization_type': data_widgets['normalization_type'].value,\n",
        "        },\n",
        "        'model': {\n",
        "            'hidden_sizes': _str_to_int_list(model_widgets['hidden_sizes'].value),\n",
        "            'dropout_prob': model_widgets['dropout_prob'].value,\n",
        "            'use_batch_norm': model_widgets['use_batch_norm'].value,\n",
        "        },\n",
        "        'training': {\n",
        "            'batch_size': training_widgets['batch_size'].value,\n",
        "            'learning_rate': training_widgets['learning_rate'].value,\n",
        "            'weight_decay': training_widgets['weight_decay'].value,\n",
        "            'num_epochs': training_widgets['num_epochs'].value,\n",
        "            'early_stopping_patience': training_widgets['early_stopping_patience'].value,\n",
        "            'eval_interval': training_widgets['eval_interval'].value,\n",
        "            'device': training_widgets['device'].value,\n",
        "        },\n",
        "        'eval': {\n",
        "            'top_m_values': _str_to_int_list(eval_widgets['top_m_values'].value),\n",
        "        },\n",
        "        'probe_bank_methods': list(comparison_widgets['probe_bank_methods'].value),\n",
        "        'models_to_run': list(comparison_widgets['models_to_run'].value),\n",
        "        'plots': list(comparison_widgets['plots'].value),\n",
        "        'compare_metrics': _str_to_int_list(comparison_widgets['compare_metrics'].value),\n",
        "    }\n",
        "\n",
        "config_preview = widgets.Output()\n",
        "def show_config_preview(_=None):\n",
        "    config_preview.clear_output()\n",
        "    with config_preview:\n",
        "        print(build_config_from_widgets())\n",
        "\n",
        "preview_button = widgets.Button(description='Preview Config')\n",
        "preview_button.on_click(show_config_preview)\n",
        "display(preview_button, config_preview)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ---- EXPERIMENT HELPERS ----\n",
        "def run_single_model(model_name, overrides):\n",
        "    config = get_config(**overrides)\n",
        "\n",
        "    torch.manual_seed(config.data.seed)\n",
        "    np.random.seed(config.data.seed)\n",
        "\n",
        "    probe_bank = generate_probe_bank(\n",
        "        N=config.system.N,\n",
        "        K=config.system.K,\n",
        "        seed=config.data.seed,\n",
        "        phase_mode=config.system.phase_mode,\n",
        "        phase_bits=config.system.phase_bits,\n",
        "        probe_bank_method=config.system.probe_bank_method,\n",
        "    )\n",
        "\n",
        "    train_loader, val_loader, test_loader, metadata = create_dataloaders(config, probe_bank)\n",
        "    model = MODEL_REGISTRY[model_name](config)\n",
        "    model, history = train(model, train_loader, val_loader, config, metadata)\n",
        "\n",
        "    results = evaluate_model(\n",
        "        model,\n",
        "        test_loader,\n",
        "        config,\n",
        "        metadata['test_powers_full'],\n",
        "        metadata['test_labels'],\n",
        "        metadata['test_observed_indices'],\n",
        "        metadata['test_optimal_powers'],\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'name': f\"{model_name} | {config.system.probe_bank_method}\",\n",
        "        'model': model,\n",
        "        'history': history,\n",
        "        'results': results,\n",
        "        'config': config,\n",
        "    }\n",
        "\n",
        "def run_comparison(config_dict):\n",
        "    overrides = {\n",
        "        'system': config_dict['system'],\n",
        "        'data': config_dict['data'],\n",
        "        'training': config_dict['training'],\n",
        "        'model': config_dict['model'],\n",
        "        'eval': config_dict['eval'],\n",
        "    }\n",
        "    results = []\n",
        "    for method in config_dict['probe_bank_methods']:\n",
        "        overrides['system']['probe_bank_method'] = method\n",
        "        for model_name in config_dict['models_to_run']:\n",
        "            results.append(run_single_model(model_name, overrides))\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ---- PLOT REGISTRY ----\n",
        "def plot_model_metric_bars(runs, metrics):\n",
        "    names = [r['name'] for r in runs]\n",
        "    x = np.arange(len(names))\n",
        "    width = 0.2\n",
        "    fig, ax = plt.subplots(figsize=(10, 5))\n",
        "    for i, metric in enumerate(metrics):\n",
        "        values = [getattr(r['results'], metric) for r in runs]\n",
        "        ax.bar(x + i * width, values, width, label=metric)\n",
        "    ax.set_xticks(x + width * (len(metrics) - 1) / 2)\n",
        "    ax.set_xticklabels(names, rotation=15)\n",
        "    ax.set_ylabel('Metric value')\n",
        "    ax.set_title('Model Comparison')\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_eta_boxplot(runs):\n",
        "    labels = [r['name'] for r in runs]\n",
        "    data = [r['results'].eta_top1_distribution for r in runs]\n",
        "    fig, ax = plt.subplots(figsize=(10, 5))\n",
        "    ax.boxplot(data, labels=labels, showmeans=True)\n",
        "    ax.set_ylabel('\u03b7 (Top-1)')\n",
        "    ax.set_title('\u03b7 Distribution by Model')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "PLOT_REGISTRY = {\n",
        "    'training_history': lambda run: plot_training_history(run['history']),\n",
        "    'eta_distribution': lambda run: plot_eta_distribution(run['results']),\n",
        "    'top_m_comparison': lambda run: plot_top_m_comparison(run['results']),\n",
        "    'baseline_comparison': lambda run: plot_baseline_comparison(run['results']),\n",
        "    'model_metric_bars': lambda runs, metrics: plot_model_metric_bars(runs, metrics),\n",
        "    'eta_boxplot': plot_eta_boxplot,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ---- RUN + PLOT (click) ----\n",
        "run_button = widgets.Button(description='Run Experiments', button_style='success')\n",
        "run_output = widgets.Output()\n",
        "\n",
        "def on_run_clicked(_):\n",
        "    run_output.clear_output()\n",
        "    with run_output:\n",
        "        config_dict = build_config_from_widgets()\n",
        "        runs = run_comparison(config_dict)\n",
        "        print('Finished runs:', [r['name'] for r in runs])\n",
        "\n",
        "        for plot_name in config_dict['plots']:\n",
        "            if plot_name in {'model_metric_bars', 'eta_boxplot'}:\n",
        "                if plot_name == 'model_metric_bars':\n",
        "                    PLOT_REGISTRY[plot_name](runs, config_dict['compare_metrics'])\n",
        "                else:\n",
        "                    PLOT_REGISTRY[plot_name](runs)\n",
        "            else:\n",
        "                for run in runs:\n",
        "                    PLOT_REGISTRY[plot_name](run)\n",
        "\n",
        "run_button.on_click(on_run_clicked)\n",
        "display(run_button, run_output)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}