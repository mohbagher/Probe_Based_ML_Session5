{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visual Experiment Studio (Fully Customizable)\n",
        "\n",
        "This notebook provides a **visual, widget-based interface** to configure every parameter\n",
        "in the project, select probe-bank methods (random, Hadamard, Sobol, Halton), choose\n",
        "models to compare, and render multiple plot types.\n",
        "\n",
        "Use the controls below, then click **Run Experiments**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "from config import get_config\n",
        "from data_generation import generate_probe_bank, create_dataloaders\n",
        "from training import train\n",
        "from evaluation import evaluate_model\n",
        "from utils import (\n",
        "    plot_training_history,\n",
        "    plot_eta_distribution,\n",
        "    plot_top_m_comparison,\n",
        "    plot_baseline_comparison,\n",
        ")\n",
        "from model import LimitedProbingMLP\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---- MODEL REGISTRY (add your own builders here) ----\n",
        "def build_mlp_default(config):\n",
        "    return LimitedProbingMLP(\n",
        "        K=config.system.K,\n",
        "        hidden_sizes=config.model.hidden_sizes,\n",
        "        dropout_prob=config.model.dropout_prob,\n",
        "        use_batch_norm=config.model.use_batch_norm,\n",
        "    )\n",
        "\n",
        "def build_mlp_shallow(config):\n",
        "    return LimitedProbingMLP(\n",
        "        K=config.system.K,\n",
        "        hidden_sizes=[128],\n",
        "        dropout_prob=0.0,\n",
        "        use_batch_norm=False,\n",
        "    )\n",
        "\n",
        "def build_linear(config):\n",
        "    input_size = 2 * config.system.K\n",
        "    output_size = config.system.K\n",
        "    return torch.nn.Sequential(torch.nn.Linear(input_size, output_size))\n",
        "\n",
        "MODEL_REGISTRY = {\n",
        "    'mlp_default': build_mlp_default,\n",
        "    'mlp_shallow': build_mlp_shallow,\n",
        "    'linear': build_linear,\n",
        "}\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---- VISUAL CONFIG PANEL ----\n",
        "# System\n",
        "system_N = widgets.IntText(value=32, description='N')\n",
        "system_K = widgets.IntText(value=64, description='K')\n",
        "system_M = widgets.IntText(value=8, description='M')\n",
        "system_P_tx = widgets.FloatText(value=1.0, description='P_tx')\n",
        "system_sigma_h = widgets.FloatText(value=1.0, description='sigma_h_sq')\n",
        "system_sigma_g = widgets.FloatText(value=1.0, description='sigma_g_sq')\n",
        "system_phase_mode = widgets.Dropdown(options=['continuous', 'discrete'], value='continuous', description='phase_mode')\n",
        "system_phase_bits = widgets.IntText(value=3, description='phase_bits')\n",
        "system_probe_bank_mode = widgets.Dropdown(options=['random', 'hadamard', 'sobol', 'halton'], value='random', description='probe_bank')\n",
        "system_qmc_scramble = widgets.Checkbox(value=True, description='qmc_scramble')\n",
        "\n",
        "system_box = widgets.VBox([\n",
        "    system_N, system_K, system_M, system_P_tx,\n",
        "    system_sigma_h, system_sigma_g,\n",
        "    system_phase_mode, system_phase_bits,\n",
        "    system_probe_bank_mode, system_qmc_scramble\n",
        "])\n",
        "\n",
        "# Data\n",
        "data_n_train = widgets.IntText(value=3000, description='n_train')\n",
        "data_n_val = widgets.IntText(value=600, description='n_val')\n",
        "data_n_test = widgets.IntText(value=600, description='n_test')\n",
        "data_seed = widgets.IntText(value=42, description='seed')\n",
        "data_normalize = widgets.Checkbox(value=True, description='normalize_input')\n",
        "data_norm_type = widgets.Dropdown(options=['mean', 'std', 'log'], value='mean', description='normalization_type')\n",
        "data_box = widgets.VBox([\n",
        "    data_n_train, data_n_val, data_n_test, data_seed, data_normalize, data_norm_type\n",
        "])\n",
        "\n",
        "# Model\n",
        "model_hidden = widgets.Text(value='256,128', description='hidden_sizes')\n",
        "model_dropout = widgets.FloatText(value=0.1, description='dropout_prob')\n",
        "model_batch_norm = widgets.Checkbox(value=True, description='use_batch_norm')\n",
        "model_box = widgets.VBox([model_hidden, model_dropout, model_batch_norm])\n",
        "\n",
        "# Training\n",
        "train_batch = widgets.IntText(value=128, description='batch_size')\n",
        "train_lr = widgets.FloatText(value=1e-3, description='learning_rate')\n",
        "train_weight_decay = widgets.FloatText(value=1e-5, description='weight_decay')\n",
        "train_epochs = widgets.IntText(value=10, description='num_epochs')\n",
        "train_early = widgets.IntText(value=15, description='early_stopping_patience')\n",
        "train_eval_interval = widgets.IntText(value=1, description='eval_interval')\n",
        "train_device = widgets.Dropdown(options=['cpu', 'cuda'], value='cpu', description='device')\n",
        "train_box = widgets.VBox([\n",
        "    train_batch, train_lr, train_weight_decay, train_epochs,\n",
        "    train_early, train_eval_interval, train_device\n",
        "])\n",
        "\n",
        "# Eval\n",
        "eval_top_m = widgets.Text(value='1,2,4,8', description='top_m_values')\n",
        "eval_box = widgets.VBox([eval_top_m])\n",
        "\n",
        "# Experiment selection\n",
        "model_select = widgets.SelectMultiple(\n",
        "    options=list(MODEL_REGISTRY.keys()),\n",
        "    value=('mlp_default',),\n",
        "    description='models',\n",
        ")\n",
        "plot_select = widgets.SelectMultiple(\n",
        "    options=[\n",
        "        'training_history', 'eta_distribution', 'top_m_comparison',\n",
        "        'baseline_comparison', 'model_metric_bars', 'eta_boxplot'\n",
        "    ],\n",
        "    value=('model_metric_bars', 'eta_boxplot'),\n",
        "    description='plots',\n",
        ")\n",
        "compare_metrics = widgets.Text(value='accuracy_top1,eta_top1,eta_top2', description='compare_metrics')\n",
        "experiment_box = widgets.VBox([model_select, plot_select, compare_metrics])\n",
        "\n",
        "# Layout\n",
        "tabs = widgets.Tab(children=[system_box, data_box, model_box, train_box, eval_box, experiment_box])\n",
        "titles = ['System', 'Data', 'Model', 'Training', 'Eval', 'Experiment']\n",
        "for i, title in enumerate(titles):\n",
        "    tabs.set_title(i, title)\n",
        "\n",
        "display(tabs)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---- RUNNER + PLOTS ----\n",
        "def _parse_int_list(text):\n",
        "    return [int(x.strip()) for x in text.split(',') if x.strip()]\n",
        "\n",
        "def _parse_float_list(text):\n",
        "    return [float(x.strip()) for x in text.split(',') if x.strip()]\n",
        "\n",
        "def build_config_from_widgets():\n",
        "    return {\n",
        "        'system': {\n",
        "            'N': system_N.value,\n",
        "            'K': system_K.value,\n",
        "            'M': system_M.value,\n",
        "            'P_tx': system_P_tx.value,\n",
        "            'sigma_h_sq': system_sigma_h.value,\n",
        "            'sigma_g_sq': system_sigma_g.value,\n",
        "            'phase_mode': system_phase_mode.value,\n",
        "            'phase_bits': system_phase_bits.value,\n",
        "            'probe_bank_mode': system_probe_bank_mode.value,\n",
        "            'qmc_scramble': system_qmc_scramble.value,\n",
        "        },\n",
        "        'data': {\n",
        "            'n_train': data_n_train.value,\n",
        "            'n_val': data_n_val.value,\n",
        "            'n_test': data_n_test.value,\n",
        "            'seed': data_seed.value,\n",
        "            'normalize_input': data_normalize.value,\n",
        "            'normalization_type': data_norm_type.value,\n",
        "        },\n",
        "        'model': {\n",
        "            'hidden_sizes': _parse_int_list(model_hidden.value),\n",
        "            'dropout_prob': model_dropout.value,\n",
        "            'use_batch_norm': model_batch_norm.value,\n",
        "        },\n",
        "        'training': {\n",
        "            'batch_size': train_batch.value,\n",
        "            'learning_rate': train_lr.value,\n",
        "            'weight_decay': train_weight_decay.value,\n",
        "            'num_epochs': train_epochs.value,\n",
        "            'early_stopping_patience': train_early.value,\n",
        "            'eval_interval': train_eval_interval.value,\n",
        "            'device': train_device.value,\n",
        "        },\n",
        "        'eval': {\n",
        "            'top_m_values': _parse_int_list(eval_top_m.value),\n",
        "        },\n",
        "        'models_to_run': list(model_select.value),\n",
        "        'plots': list(plot_select.value),\n",
        "        'compare_metrics': [m.strip() for m in compare_metrics.value.split(',') if m.strip()],\n",
        "    }\n",
        "\n",
        "def run_single_model(model_name, overrides):\n",
        "    config = get_config(**overrides)\n",
        "    torch.manual_seed(config.data.seed)\n",
        "    np.random.seed(config.data.seed)\n",
        "\n",
        "    probe_bank = generate_probe_bank(\n",
        "        N=config.system.N,\n",
        "        K=config.system.K,\n",
        "        seed=config.data.seed,\n",
        "        phase_mode=config.system.phase_mode,\n",
        "        phase_bits=config.system.phase_bits,\n",
        "        probe_bank_mode=config.system.probe_bank_mode,\n",
        "        qmc_scramble=config.system.qmc_scramble,\n",
        "    )\n",
        "\n",
        "    train_loader, val_loader, test_loader, metadata = create_dataloaders(\n",
        "        config, probe_bank\n",
        "    )\n",
        "\n",
        "    model = MODEL_REGISTRY[model_name](config)\n",
        "    model, history = train(model, train_loader, val_loader, config, metadata)\n",
        "\n",
        "    results = evaluate_model(\n",
        "        model,\n",
        "        test_loader,\n",
        "        config,\n",
        "        metadata['test_powers_full'],\n",
        "        metadata['test_labels'],\n",
        "        metadata['test_observed_indices'],\n",
        "        metadata['test_optimal_powers'],\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'name': model_name,\n",
        "        'model': model,\n",
        "        'history': history,\n",
        "        'results': results,\n",
        "        'config': config,\n",
        "    }\n",
        "\n",
        "def run_comparison(config_dict):\n",
        "    overrides = {\n",
        "        'system': config_dict['system'],\n",
        "        'data': config_dict['data'],\n",
        "        'training': config_dict['training'],\n",
        "        'model': config_dict['model'],\n",
        "        'eval': config_dict['eval'],\n",
        "    }\n",
        "    runs = []\n",
        "    for model_name in config_dict['models_to_run']:\n",
        "        runs.append(run_single_model(model_name, overrides))\n",
        "    return runs\n",
        "\n",
        "def plot_model_metric_bars(runs, metrics):\n",
        "    names = [r['name'] for r in runs]\n",
        "    x = np.arange(len(names))\n",
        "    width = 0.2\n",
        "    fig, ax = plt.subplots(figsize=(10, 5))\n",
        "    for i, metric in enumerate(metrics):\n",
        "        values = [getattr(r['results'], metric) for r in runs]\n",
        "        ax.bar(x + i * width, values, width, label=metric)\n",
        "    ax.set_xticks(x + width * (len(metrics) - 1) / 2)\n",
        "    ax.set_xticklabels(names, rotation=15)\n",
        "    ax.set_ylabel('Metric value')\n",
        "    ax.set_title('Model Comparison')\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_eta_boxplot(runs):\n",
        "    labels = [r['name'] for r in runs]\n",
        "    data = [r['results'].eta_top1_distribution for r in runs]\n",
        "    fig, ax = plt.subplots(figsize=(10, 5))\n",
        "    ax.boxplot(data, labels=labels, showmeans=True)\n",
        "    ax.set_ylabel('\u03b7 (Top-1)')\n",
        "    ax.set_title('\u03b7 Distribution by Model')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "PLOT_REGISTRY = {\n",
        "    'training_history': lambda run: plot_training_history(run['history']),\n",
        "    'eta_distribution': lambda run: plot_eta_distribution(run['results']),\n",
        "    'top_m_comparison': lambda run: plot_top_m_comparison(run['results']),\n",
        "    'baseline_comparison': lambda run: plot_baseline_comparison(run['results']),\n",
        "    'model_metric_bars': lambda runs, metrics: plot_model_metric_bars(runs, metrics),\n",
        "    'eta_boxplot': plot_eta_boxplot,\n",
        "}\n",
        "\n",
        "run_button = widgets.Button(description='Run Experiments', button_style='success')\n",
        "output = widgets.Output()\n",
        "\n",
        "def on_run_clicked(_):\n",
        "    with output:\n",
        "        clear_output()\n",
        "        config_dict = build_config_from_widgets()\n",
        "        runs = run_comparison(config_dict)\n",
        "\n",
        "        for plot_name in config_dict['plots']:\n",
        "            if plot_name in {'model_metric_bars', 'eta_boxplot'}:\n",
        "                if plot_name == 'model_metric_bars':\n",
        "                    PLOT_REGISTRY[plot_name](runs, config_dict['compare_metrics'])\n",
        "                else:\n",
        "                    PLOT_REGISTRY[plot_name](runs)\n",
        "            else:\n",
        "                for run in runs:\n",
        "                    PLOT_REGISTRY[plot_name](run)\n",
        "\n",
        "run_button.on_click(on_run_clicked)\n",
        "display(run_button, output)\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}