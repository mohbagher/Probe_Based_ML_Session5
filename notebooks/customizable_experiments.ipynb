{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fully Customizable RIS Probe-Based ML Notebook\n",
        "\n",
        "This notebook exposes a **single config dictionary** to control:\n",
        "- Continuous vs. discrete phases (and quantization bits)\n",
        "- Which models run\n",
        "- Which comparisons and plots are generated\n",
        "- Training/data sizes for fast iteration\n",
        "\n",
        "Keep changes **short** by editing only the `CONFIG` dict.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from config import get_config\n",
        "from data_generation import generate_probe_bank, create_dataloaders\n",
        "from training import train\n",
        "from evaluation import evaluate_model\n",
        "from utils import (\n",
        "    plot_training_history,\n",
        "    plot_eta_distribution,\n",
        "    plot_top_m_comparison,\n",
        "    plot_baseline_comparison,\n",
        ")\n",
        "from model import LimitedProbingMLP\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---- CONFIGURATION (edit only this dict for quick customization) ----\n",
        "CONFIG = {\n",
        "    # System + data\n",
        "    'system': {\n",
        "        'N': 32,\n",
        "        'K': 64,\n",
        "        'M': 8,\n",
        "        'phase_mode': 'continuous',  # 'continuous' or 'discrete'\n",
        "        'phase_bits': 3,            # used only when phase_mode='discrete'\n",
        "    },\n",
        "    'data': {\n",
        "        'n_train': 3000,\n",
        "        'n_val': 600,\n",
        "        'n_test': 600,\n",
        "        'seed': 42,\n",
        "        'normalize_input': True,\n",
        "    },\n",
        "    # Training\n",
        "    'training': {\n",
        "        'num_epochs': 10,\n",
        "        'batch_size': 128,\n",
        "        'learning_rate': 1e-3,\n",
        "    },\n",
        "    # Default MLP config\n",
        "    'model': {\n",
        "        'hidden_sizes': [256, 128],\n",
        "        'dropout_prob': 0.1,\n",
        "        'use_batch_norm': True,\n",
        "    },\n",
        "    # Comparison controls\n",
        "    'models_to_run': [\n",
        "        'mlp_default',\n",
        "        'mlp_shallow',\n",
        "        'linear',\n",
        "    ],\n",
        "    'plots': [\n",
        "        'training_history',\n",
        "        'eta_distribution',\n",
        "        'top_m_comparison',\n",
        "        'baseline_comparison',\n",
        "        'model_metric_bars',\n",
        "        'eta_boxplot',\n",
        "    ],\n",
        "    # Metrics shown in bar comparison\n",
        "    'compare_metrics': ['accuracy_top1', 'eta_top1', 'eta_top2'],\n",
        "}\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---- MODEL REGISTRY (add your own builders here) ----\n",
        "def build_mlp_default(config):\n",
        "    return LimitedProbingMLP(\n",
        "        K=config.system.K,\n",
        "        hidden_sizes=config.model.hidden_sizes,\n",
        "        dropout_prob=config.model.dropout_prob,\n",
        "        use_batch_norm=config.model.use_batch_norm,\n",
        "    )\n",
        "\n",
        "def build_mlp_shallow(config):\n",
        "    return LimitedProbingMLP(\n",
        "        K=config.system.K,\n",
        "        hidden_sizes=[128],\n",
        "        dropout_prob=0.0,\n",
        "        use_batch_norm=False,\n",
        "    )\n",
        "\n",
        "def build_linear(config):\n",
        "    # Simple linear classifier as a fast baseline\n",
        "    input_size = 2 * config.system.K\n",
        "    output_size = config.system.K\n",
        "    return torch.nn.Sequential(torch.nn.Linear(input_size, output_size))\n",
        "\n",
        "MODEL_REGISTRY = {\n",
        "    'mlp_default': build_mlp_default,\n",
        "    'mlp_shallow': build_mlp_shallow,\n",
        "    'linear': build_linear,\n",
        "}\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---- EXPERIMENT HELPERS ----\n",
        "def build_config(overrides):\n",
        "    return get_config(**overrides)\n",
        "\n",
        "def run_single_model(model_name, overrides):\n",
        "    config = build_config(overrides)\n",
        "\n",
        "    # Set seeds for reproducibility\n",
        "    torch.manual_seed(config.data.seed)\n",
        "    np.random.seed(config.data.seed)\n",
        "\n",
        "    probe_bank = generate_probe_bank(\n",
        "        N=config.system.N,\n",
        "        K=config.system.K,\n",
        "        seed=config.data.seed,\n",
        "        phase_mode=config.system.phase_mode,\n",
        "        phase_bits=config.system.phase_bits,\n",
        "    )\n",
        "\n",
        "    train_loader, val_loader, test_loader, metadata = create_dataloaders(\n",
        "        config, probe_bank\n",
        "    )\n",
        "\n",
        "    model = MODEL_REGISTRY[model_name](config)\n",
        "    model, history = train(model, train_loader, val_loader, config, metadata)\n",
        "\n",
        "    results = evaluate_model(\n",
        "        model,\n",
        "        test_loader,\n",
        "        config,\n",
        "        metadata['test_powers_full'],\n",
        "        metadata['test_labels'],\n",
        "        metadata['test_observed_indices'],\n",
        "        metadata['test_optimal_powers'],\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'name': model_name,\n",
        "        'model': model,\n",
        "        'history': history,\n",
        "        'results': results,\n",
        "        'config': config,\n",
        "    }\n",
        "\n",
        "def run_comparison(config_dict):\n",
        "    overrides = {\n",
        "        'system': config_dict['system'],\n",
        "        'data': config_dict['data'],\n",
        "        'training': config_dict['training'],\n",
        "        'model': config_dict['model'],\n",
        "    }\n",
        "    results = []\n",
        "    for model_name in config_dict['models_to_run']:\n",
        "        results.append(run_single_model(model_name, overrides))\n",
        "    return results\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---- RUN EXPERIMENTS ----\n",
        "all_runs = run_comparison(CONFIG)\n",
        "print('Finished models:', [run['name'] for run in all_runs])\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---- PLOT REGISTRY ----\n",
        "def plot_model_metric_bars(runs, metrics):\n",
        "    names = [r['name'] for r in runs]\n",
        "    x = np.arange(len(names))\n",
        "    width = 0.2\n",
        "    fig, ax = plt.subplots(figsize=(10, 5))\n",
        "    for i, metric in enumerate(metrics):\n",
        "        values = [getattr(r['results'], metric) for r in runs]\n",
        "        ax.bar(x + i * width, values, width, label=metric)\n",
        "    ax.set_xticks(x + width * (len(metrics) - 1) / 2)\n",
        "    ax.set_xticklabels(names, rotation=15)\n",
        "    ax.set_ylabel('Metric value')\n",
        "    ax.set_title('Model Comparison')\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_eta_boxplot(runs):\n",
        "    labels = [r['name'] for r in runs]\n",
        "    data = [r['results'].eta_top1_distribution for r in runs]\n",
        "    fig, ax = plt.subplots(figsize=(10, 5))\n",
        "    ax.boxplot(data, labels=labels, showmeans=True)\n",
        "    ax.set_ylabel('\u03b7 (Top-1)')\n",
        "    ax.set_title('\u03b7 Distribution by Model')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "PLOT_REGISTRY = {\n",
        "    'training_history': lambda run: plot_training_history(run['history']),\n",
        "    'eta_distribution': lambda run: plot_eta_distribution(run['results']),\n",
        "    'top_m_comparison': lambda run: plot_top_m_comparison(run['results']),\n",
        "    'baseline_comparison': lambda run: plot_baseline_comparison(run['results']),\n",
        "    'model_metric_bars': lambda runs: plot_model_metric_bars(runs, CONFIG['compare_metrics']),\n",
        "    'eta_boxplot': plot_eta_boxplot,\n",
        "}\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---- RENDER PLOTS ----\n",
        "for plot_name in CONFIG['plots']:\n",
        "    if plot_name in {'model_metric_bars', 'eta_boxplot'}:\n",
        "        PLOT_REGISTRY[plot_name](all_runs)\n",
        "    else:\n",
        "        # per-model plots\n",
        "        for run in all_runs:\n",
        "            PLOT_REGISTRY[plot_name](run)\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}