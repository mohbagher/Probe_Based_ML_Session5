# üìö PhD Research Dashboard - Complete User Guide

## Table of Contents
1. [System Parameters](#1-system-parameters)
2. [Model Architectures](#2-model-architectures)
3. [Training Configuration](#3-training-configuration)
4. [Evaluation Metrics](#4-evaluation-metrics)
5. [Visualization Options](#5-visualization-options)
6. [Advanced Features](#6-advanced-features)

---

## 1. System Parameters

### 1.1 N (Number of RIS Elements)

**What it is:** Physical antenna elements in the Reconfigurable Intelligent Surface.

**Technical Background:**
- RIS is an array of passive reflecting elements
- Each element can independently control phase shift
- More elements ‚Üí higher beamforming gain but more complexity

**Mathematical Impact:**
- Channel dimension: h ‚àà ‚ÑÇ^N (BS-RIS), g ‚àà ‚ÑÇ^N (RIS-UE)
- Optimal beamforming gain: O(N¬≤)
- Computational complexity: O(NK) for all probe evaluations

**When to use different values:**
- **N=8-16:** Quick prototyping, low-complexity scenarios
- **N=32:** Standard research baseline (good balance)
- **N=64-128:** Realistic deployments, high-performance systems
- **N=256+:** Advanced research, asymptotic analysis

**Trade-offs:**
- ‚úÖ Larger N: Better performance, higher capacity
- ‚ùå Larger N: More hardware cost, higher training time, larger models

**Practical Recommendations:**
- Start with N=32 for initial experiments
- Use N=16 for quick debugging
- Use N=64+ for final paper results

---

### 1.2 K (Codebook Size)

**What it is:** Number of predefined phase configurations (probes) in your library.

**Technical Background:**
- Codebook = set of K possible RIS configurations
- Each probe k has N phase values: Œ∏‚Çñ ‚àà [0, 2œÄ)^N
- Exhaustive search: measure all K probes ‚Üí infeasible for large K
- Our approach: ML predicts best probe from limited measurements

**Mathematical Impact:**
- Search space: 2^(N¬∑bits) possible configurations (discrete phase)
- Codebook reduces to K << 2^(N¬∑bits)
- Classification problem: predict best among K options

**When to use different values:**
- **K=16-32:** Simple scenarios, proof-of-concept
- **K=64:** Standard baseline (used in most papers)
- **K=128-256:** Complex environments, higher capacity
- **K=512+:** Research on scalability limits

**Trade-offs:**
- ‚úÖ Larger K: More optimization freedom, better optimal performance
- ‚ùå Larger K: Harder ML task, needs more training data, slower inference

**Design Considerations:**
- K should be large enough to contain near-optimal solutions
- K vs N ratio: typically K = 2N to 4N is good
- K must be >> M (sensing budget) for meaningful ML problem

---

### 1.3 M (Sensing Budget)

**What it is:** Number of probes you can actually measure (test) before making a decision.

**Technical Background:**
- Limited probing constraint: can only try M << K probes
- Motivation: Each measurement costs time, energy, signaling overhead
- Classic trade-off: exploration vs exploitation
- M/K ratio defines problem difficulty

**Mathematical Impact:**
- Input features: only M observed powers out of K total
- Sparse observation: 1-M/K fraction missing
- ML model must infer from partial information

**When to use different values:**
- **M=2-4:** Extremely limited (hard problem, practical for fast systems)
- **M=8:** Standard baseline (good for research)
- **M=16-32:** More information available (easier ML task)
- **M=K/2:** Moderate observation (for comparison with full knowledge)

**Trade-offs:**
- ‚úÖ Larger M: Easier ML problem, better predictions, safer decisions
- ‚ùå Larger M: Higher overhead, slower adaptation, more energy

**Key Relationships:**
- M ‚Üí 1: ML must extrapolate heavily (hard)
- M ‚Üí K: Approaches full information (easy but defeats purpose)
- Typical sweet spot: M = K/8 to K/4

**Performance Metrics:**
- Œ∑ (eta) = P_predicted / P_best: measures how close to optimal
- Best observed baseline: max power among M measured probes
- ML improvement: how much better than best observed

---

## 2. Model Architectures

### 2.1 MLP (Multi-Layer Perceptron)

**Architecture:** Fully connected neural network.

**How it works:**
```
Input (2K) ‚Üí Dense(512) ‚Üí ReLU ‚Üí Dropout ‚Üí 
           ‚Üí Dense(256) ‚Üí ReLU ‚Üí Dropout ‚Üí
           ‚Üí Dense(128) ‚Üí ReLU ‚Üí Dropout ‚Üí
           ‚Üí Dense(K) ‚Üí Softmax
```

**Technical Background:**
- Universal approximation theorem: can learn any continuous function
- Each layer transforms: h = ReLU(Wx + b)
- Learns hierarchical representations
- No spatial/temporal structure assumed

**Advantages:**
- ‚úÖ Simple, well-understood
- ‚úÖ Fast training and inference
- ‚úÖ Works well for tabular/vector data
- ‚úÖ Few hyperparameters to tune
- ‚úÖ Good baseline for comparison

**Disadvantages:**
- ‚ùå No built-in structure awareness
- ‚ùå Treats all inputs independently
- ‚ùå Can overfit on small datasets

**When to use:**
- Default choice for vector inputs
- When data has no obvious structure
- When you need fast training
- For establishing baselines

**Hyperparameters:**
- **Hidden sizes:** [512, 256, 128] is good default
  - Deeper = more capacity but harder to train
  - Wider = more parameters but parallelizable
- **Dropout:** 0.1-0.3 typical range
  - Higher if overfitting
  - Lower if underfitting
- **Batch norm:** almost always beneficial

**Expected Performance:**
- Should achieve Œ∑ ‚âà 0.85-0.92 (M=8, K=64, N=32)
- Training time: ~2-5 min (50 epochs, CPU)
- Parameters: ~500K for [512,256,128]

---

### 2.2 CNN (Convolutional Neural Network)

**Architecture:** 1D convolutions over probe features.

**How it works:**
```
Input (2 channels √ó K length) ‚Üí 
  Conv1D(32, kernel=5) ‚Üí ReLU ‚Üí Pool ‚Üí
  Conv1D(64, kernel=5) ‚Üí ReLU ‚Üí Pool ‚Üí
  Conv1D(128, kernel=3) ‚Üí ReLU ‚Üí Pool ‚Üí
  Flatten ‚Üí Dense(256) ‚Üí Dense(K)
```

**Technical Background:**
- Exploits local correlations in probe space
- Kernel slides across probe indices
- Parameter sharing reduces model size
- Hierarchical feature extraction

**Key Concept - Why CNN for Probes:**
- Probes with similar indices may have similar powers
- Spatial structure in phase configurations
- Local patterns more important than global
- Weight sharing = inductive bias

**Advantages:**
- ‚úÖ Captures local structure
- ‚úÖ Fewer parameters than MLP (weight sharing)
- ‚úÖ Translation invariant
- ‚úÖ Good for structured probe banks (e.g., Hadamard)

**When to use:**
- Structured codebooks (Hadamard, Sobol)
- When probe order matters
- Larger K (spatial structure more apparent)

**Hyperparameters:**
- **Num conv layers:** 3-5 typical
- **Channels:** [32, 64, 128] progressive increase
- **Kernel size:** 3-7 (odd numbers)
  - Larger kernel = more context
  - Smaller kernel = more local

**Expected Performance:**
- Œ∑ ‚âà 0.87-0.94 for structured probes
- Best for: Hadamard, Sobol probe methods

---

### 2.3 LSTM (Long Short-Term Memory)

**Architecture:** Recurrent network treating probes as sequence.

**How it works:**
```
Input sequence (K timesteps, 2 features) ‚Üí
  BiLSTM(128, 2 layers) ‚Üí
  Flatten ‚Üí Dense(256) ‚Üí Dense(K)
```

**Technical Background:**
- Treats K probes as temporal sequence
- Each probe = 1 timestep with [power, mask] features
- LSTM cell: can remember long-term dependencies
- Bidirectional: processes sequence both directions

**Advantages:**
- ‚úÖ Models sequential dependencies
- ‚úÖ Handles variable-length sequences
- ‚úÖ Captures long-range correlations
- ‚úÖ Bidirectional sees full context

**Disadvantages:**
- ‚ùå Sequential computation (slow)
- ‚ùå Can't parallelize across time
- ‚ùå Needs more data than MLP

**When to use:**
- Ordered probe sequences
- Time-varying channels
- When probe history matters

**Expected Performance:**
- Œ∑ ‚âà 0.84-0.91
- Slower training (5-10√ó vs MLP)

---

### 2.4 GRU (Gated Recurrent Unit)

**Architecture:** Simplified LSTM with fewer gates.

**Advantages over LSTM:**
- ‚úÖ Faster training (fewer parameters)
- ‚úÖ Similar performance in many cases
- ‚úÖ Less prone to overfitting

**When to use:**
- Similar to LSTM but want faster training
- When LSTM shows overfitting

---

### 2.5 Attention MLP

**Architecture:** MLP with multi-head attention mechanism.

**How it works:**
```
Input ‚Üí Dense(512) ‚Üí
  MultiHeadAttention(4 heads) ‚Üí
  Dense(256) ‚Üí Dense(128) ‚Üí Dense(K)
```

**Why combine MLP + Attention:**
- Attention finds important features
- MLP processes attended features
- Best of both worlds

**Expected Performance:**
- Œ∑ ‚âà 0.88-0.94
- Good balance: simpler than Transformer, better than MLP

---

### 2.6 Transformer

**Architecture:** Self-attention mechanism.

**How it works:**
```
Input (K tokens, 2 features) ‚Üí
  Embedding ‚Üí Positional Encoding ‚Üí
  Transformer Encoder (3 layers):
    MultiHeadAttention(8 heads) ‚Üí
    FeedForward(512) ‚Üí
  ‚Üí  Global Pool ‚Üí Dense(K)
```

**Technical Background:**
- Attention: learns which probes are most relevant
- Self-attention: each probe attends to all others
- Position-agnostic: doesn't assume order
- Parallel computation: very fast on GPU

**Advantages:**
- ‚úÖ Most powerful architecture (SOTA)
- ‚úÖ Learns complex relationships
- ‚úÖ Parallel computation (fast on GPU)
- ‚úÖ Interpretable attention weights

**Disadvantages:**
- ‚ùå Most parameters (largest model)
- ‚ùå Needs more data
- ‚ùå Can overfit easily

**When to use:**
- Large datasets (50K+ samples)
- Large K (128+)
- GPU available
- Pushing state-of-the-art

**Expected Performance:**
- Œ∑ ‚âà 0.90-0.96 (best overall with enough data)
- Needs: 50K+ training samples

---

### 2.7 ResNet-Style MLP

**Architecture:** MLP with skip connections.

**How it works:**
```
Input ‚Üí Dense(512) ‚Üí [ResBlock ‚Üí ResBlock ‚Üí ...] ‚Üí Dense(K)

ResBlock:
  x ‚Üí Dense ‚Üí ReLU ‚Üí Dense ‚Üí (+) ‚Üí ReLU
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò (skip)
```

**Advantages:**
- ‚úÖ Can go much deeper (10+ layers)
- ‚úÖ Faster convergence
- ‚úÖ Less prone to vanishing gradients

**When to use:**
- Want deeper models
- Plain MLP plateaus

---

### 2.8 Hybrid CNN-LSTM

**Architecture:** CNN for feature extraction, LSTM for sequence modeling.

**Use Case:**
- Time-varying channels
- Structured + sequential patterns

**Expected Performance:**
- Œ∑ ‚âà 0.86-0.92 for time-varying scenarios

---

## 3. Training Configuration

### 3.1 Optimizers

#### Adam (Adaptive Moment Estimation)

**Default choice** - Works well out-of-box.

**Hyperparameters:**
- **Learning rate:** 1e-4 to 1e-2 (start with 1e-3)
- **Œ≤‚ÇÅ:** 0.9 (typical, rarely changed)
- **Œ≤‚ÇÇ:** 0.999 (typical, rarely changed)

**When to use:**
- Default choice (90% of cases)
- Quick prototyping

#### AdamW

**Better regularization** - Decoupled weight decay.

**When to use:**
- Large models (Transformer)
- Need better generalization

#### SGD

**Classic optimizer** - Better generalization but needs tuning.

**Hyperparameters:**
- **Learning rate:** 0.01-0.1 (much higher than Adam!)
- **Momentum:** 0.9-0.99

**When to use:**
- Final paper results
- Care about generalization
- Use with good scheduler

---

### 3.2 Learning Rate Schedulers

#### ReduceLROnPlateau

**Default choice** - Automatic adaptation.

**Parameters:**
- **Patience:** how many epochs to wait
- **Factor:** multiply LR by this (e.g., 0.5)

#### CosineAnnealing

**Smooth decay** - LR follows cosine curve.

**When to use:**
- Know total epochs
- Want smooth schedule

#### OneCycleLR

**SOTA schedule** - Combines warmup + annealing.

**When to use:**
- Want fastest training
- SGD optimizer

---

### 3.3 Loss Functions

#### CrossEntropy

**Standard choice** for classification.

#### FocalLoss

**For imbalanced classes** - Focuses on hard examples.

#### LabelSmoothing

**Regularization technique** - Prevents overconfidence.

**When to use:**
- Model overfitting
- Want better calibration

---

## 4. Evaluation Metrics

### Top-K Accuracy

Fraction of samples where oracle best probe is in top-K predictions.

- **Top-1:** Most important (exact match)
- **Top-2, Top-4, Top-8:** Show prediction quality

### Eta (Œ∑) - Power Ratio

Œ∑ = P_predicted / P_optimal

- **Œ∑ = 1.0:** Perfect prediction
- **Œ∑ ‚âà 0.9:** Excellent (90% of optimal power)
- **Œ∑ ‚âà 0.8:** Good (80% of optimal power)
- **Œ∑ < 0.7:** Poor performance

### Baseline Comparisons

- **Random-1:** Pick 1 probe randomly from K
- **Random-M:** Best of M random probes
- **Best-Observed:** Best among actually measured M probes
- **Oracle:** True best probe (upper bound)

---

## 5. Visualization Options

### Training Plots

- **Training Curves:** Loss and accuracy over epochs
- **Learning Rate Schedule:** LR evolution
- **Gradient Flow:** Gradient magnitudes per layer

### Performance Plots

- **Eta Distribution:** Histogram/PDF of Œ∑ values
- **CDF:** Cumulative distribution function
- **Box/Violin Plot:** Statistical distribution comparison
- **Bar/Scatter:** Model comparison

### Probe Analysis

- **Probe Heatmap:** Phase configurations visualization
- **Correlation Matrix:** Probe similarity
- **Diversity Analysis:** Probe bank quality metrics

### Advanced Plots

- **3D Surface:** Performance vs two parameters
- **ROC/Precision-Recall:** Classification metrics
- **Confusion Matrix:** Prediction patterns
- **Convergence Analysis:** Multi-model comparison

---

## 6. Advanced Features

### Multi-Model Comparison

Compare multiple architectures in one run:
- Enable "Multi-Model Comparison Mode"
- Select models to compare
- Get side-by-side performance metrics

### Multi-Seed Runs

Statistical analysis with confidence intervals:
- Enable "Multi-Seed Runs"
- Set number of seeds
- Get mean ¬± std results

### Preset Configurations

Quick start with optimized presets:
- **Quick Test:** Fast configuration for testing
- **Standard Research:** Default research setup
- **High Capacity:** Large-scale deployment
- **Limited Budget:** Extreme sensing constraint

### Configuration Management

- **Save Config:** Export current settings to JSON/YAML
- **Load Config:** Import previously saved settings
- **Validation:** Auto-check for common issues

---

## Quick Start Examples

### Example 1: Basic Experiment
```python
1. Set N=32, K=64, M=8
2. Choose MLP model
3. Default training settings
4. Click "RUN EXPERIMENT"
```

### Example 2: Model Comparison
```python
1. Enable "Multi-Model Comparison Mode"
2. Select: MLP, CNN, LSTM, Transformer
3. Set epochs=50
4. Click "RUN EXPERIMENT"
5. View comparison plots
```

### Example 3: Statistical Analysis
```python
1. Enable "Multi-Seed Runs"
2. Set num_seeds=5
3. Configure your experiment
4. Click "RUN EXPERIMENT"
5. Get mean ¬± std results
```

---

## Tips & Best Practices

### For Quick Testing
- Use N=16, K=32, M=4
- 10K training samples
- 20 epochs
- MLP model

### For Research Papers
- Use N=32-64, K=64-128
- 50K+ training samples
- 50-100 epochs
- Compare multiple models
- Multi-seed runs for error bars

### For Large-Scale
- Use N=64-128, K=128-256
- 100K+ training samples
- Transformer model
- GPU recommended

---

## Troubleshooting

### Issue: Model not converging
- **Solution:** Increase learning rate, try different optimizer

### Issue: Overfitting
- **Solution:** Increase dropout, add weight decay, more training data

### Issue: Poor performance
- **Solution:** Check M vs K ratio, try different model, more epochs

### Issue: Slow training
- **Solution:** Reduce batch size, use simpler model, check device (GPU/CPU)

---

For more details, see:
- [Developer Guide](DEVELOPER_GUIDE.md) - How to extend the system
- [Tutorial](TUTORIAL.md) - Step-by-step examples
- [API Reference](API_REFERENCE.md) - Function documentation
